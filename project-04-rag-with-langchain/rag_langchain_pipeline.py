# -*- coding: utf-8 -*-
"""Project 4: RAG with LangChain

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GY0fgihtBeTquFnm0O4Qrtm9lvYeJ0Hc

# RAG using Langchain

## Packages loading & import
"""

!pip install "langchain-core>=0.2.0,<0.3.0" \
             "langchain>=0.2.0,<0.3.0" \
             "langchain-community>=0.2.0,<0.3.0" \
             "langchain-huggingface>=0.0.3,<0.1.0" \
             "langchain-chroma>=0.1.0,<0.2.0" \
             "langchain-ollama>=0.1.0,<0.2.0" \
             "langchain-text-splitters>=0.2.0,<0.3.0" \
             "transformers>=4.39.0" \
             "accelerate>=0.28.0" \
             "sentence-transformers" \
             rank-bm25 \
             huggingface_hub \
             tqdm \
             beautifulsoup4 \

!pip install --upgrade nltk

import os
import json
import bs4
import nltk
import torch
import pickle
import numpy as np

# from pyserini.index import IndexWriter
# from pyserini.search import SimpleSearcher
from numpy.linalg import norm
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

from langchain_community.llms import Ollama
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import JinaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain.docstore.document import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import WebBaseLoader
from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer

from tqdm import tqdm

nltk.download('punkt')
nltk.download('punkt_tab')

"""## Hugging face login
- Please apply the model first: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
- If you haven't been granted access to this model, you can use other LLM model that doesn't have to apply.
- You must save the hf token otherwise you need to regenrate the token everytime.
- When using Ollama, no login is required to access and utilize the llama model.
"""

from huggingface_hub import login

hf_token = os.getenv("HF_TOKEN", "")
if hf_token:
    login(token=hf_token, add_to_git_credential=True)
else:
    print("HF_TOKEN is not set; skipping Hugging Face login.")

!huggingface-cli whoami

"""## TODO1: Set up the environment of Ollama

### Introduction to Ollama
- Ollama is a platform designed for running and managing large language models (LLMs) directly **on local devices**, providing a balance between performance, privacy, and control.
- There are also other tools support users to manage LLM on local devices and accelerate it like *vllm*, *Llamafile*, *GPT4ALL*...etc.

### Launch colabxterm
"""

# Commented out IPython magic to ensure Python compatibility.
# TODO1-1: You should install colab-xterm and launch it.
!pip install colab-xterm
# %load_ext colabxterm

# TODO1-2: You should install Ollama.
# You may need root privileges if you use a local machine instead of Colab.

!curl -fsSL https://ollama.com/install.sh | sh

# Commented out IPython magic to ensure Python compatibility.
# %xterm -e

# TODO1-3: Pull Llama3.2:1b via Ollama and start the Ollama service in the xterm
# Write your commands in the xterm:
# ollama serve
# ollama pull llama3.2:1b
# ollama serve &

"""## Ollama testing
You can test your Ollama status with the following cells.
"""

# Setting up the model that this tutorial will use
MODEL = "llama3.2:1b" # https://ollama.com/library/llama3.2:3b
EMBED_MODEL = "jinaai/jina-embeddings-v2-base-en"

# Initialize an instance of the Ollama model
llm = Ollama(model=MODEL)
# Invoke the model to generate responses
response = llm.invoke("What is the capital of Taiwan?")
print(response)

"""## Build a simple RAG system by using LangChain

### TODO2: Load the cat-facts dataset and prepare the retrieval database
"""

!wget https://huggingface.co/ngxson/demo_simple_rag_py/resolve/main/cat-facts.txt

# TODO2-1: Load the cat-facts dataset (as `refs`, which is a list of strings for all the cat facts)
# Write your code here
with open('cat-facts.txt', 'r', encoding='utf-8') as f:
    refs = [line.strip() for line in f.readlines() if line.strip()]

from langchain_core.documents import Document
docs = [Document(page_content=doc, metadata={"id": i}) for i, doc in enumerate(refs)]

# Create an embedding model
model_kwargs = {'trust_remote_code': True}
encode_kwargs = {'normalize_embeddings': False}
embeddings_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

import chromadb
from chromadb.config import Settings

vector_store = Chroma.from_documents(
    documents=docs,
    embedding=embeddings_model,
    collection_name="cat_facts"
)

from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

bm25_retriever = BM25Retriever.from_documents(docs)
bm25_retriever.k = 25

vector_retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 25}
)

base_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5]
)

model = HuggingFaceCrossEncoder(
    model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
)

compressor = CrossEncoderReranker(model=model, top_n=8)

retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

"""### Prompt setting"""

system_prompt = """You are a precise answer extraction system.

Given the context below, extract the EXACT answer to the question.

Rules:
- Extract the answer directly from the context
- Keep it concise (typically 1-10 words)
- If the answer is not in the context, output "Unknown"
- Do not add explanations or extra words

Context: {context}

Question: {input}

Answer:"""

prompt = ChatPromptTemplate.from_template(system_prompt)

"""- For the vectorspace, the common algorithm would be used like Faiss, Chroma...(https://python.langchain.com/docs/integrations/vectorstores/) to deal with the extreme huge database."""

# TODO4: Build and run the RAG system
# TODO4-1: Load the QA chain
# You should create a chain for passing a list of Documents to a model.
question_answer_chain = create_stuff_documents_chain(llm, prompt)

# TODO4-2: Create retrieval chain
# You should create retrieval chain that retrieves documents and then passes them on.
chain = create_retrieval_chain(retriever, question_answer_chain)

# Question (queries) and answer pairs
# Write your code here
# Please load the questions_answers.txt file and prepare the `queries` and `answers` lists.

queries = []
answers = []

with open('/content/questions_answers.txt', 'r', encoding='utf-8') as f:
    lines = f.readlines()
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line:
            i += 1
            continue
        question = line
        if i + 1 < len(lines):
            answer = lines[i + 1].strip()
            if question and answer:
                queries.append(question)
                answers.append(answer)
            i += 2
        else:
            i += 1
results = []
correct_count = 0
retrieved_docs_stats = []

!pip install rapidfuzz -q

import re
from rapidfuzz import fuzz

NUMBER_MAP = {
    "one": "1", "two": "2", "three": "3", "four": "4", "five": "5",
    "six": "6", "seven": "7", "eight": "8", "nine": "9", "ten": "10",
    "eleven": "11", "twelve": "12", "thirteen": "13", "fourteen": "14",
    "fifteen": "15", "sixteen": "16", "seventeen": "17", "eighteen": "18",
    "nineteen": "19", "twenty": "20", "thirty": "30", "forty": "40",
    "fifty": "50", "sixty": "60", "seventy": "70", "eighty": "80",
    "ninety": "90", "hundred": "100", "thousand": "1000",
    "million": "1000000", "billion": "1000000000",
}

FRACTION_MAP = {
    "two thirds": "2/3", "one third": "1/3", "one half": "1/2",
    "three quarters": "3/4", "one quarter": "1/4",
}

def normalize_answer(text: str) -> str:
    text = str(text).lower().strip()
    prefixes = ["the answer is", "according to", "based on", "i think"]
    for prefix in prefixes:
        if text.startswith(prefix):
            text = text[len(prefix):].strip()
    approx = ['about', 'around', 'approximately', 'roughly', 'nearly', 'up to']
    for word in approx:
        text = re.sub(rf'^{word}\s+', '', text)
    for phrase, frac in FRACTION_MAP.items():
        text = text.replace(phrase, frac)
    for word, num in NUMBER_MAP.items():
        text = re.sub(rf'\b{word}\b', num, text)
    text = re.sub(r'\b(\d0)\s+(\d)\b', lambda m: str(int(m.group(1)) + int(m.group(2))), text)
    text = re.sub(r'\s+to\s+', ' ', text)
    text = re.sub(r'[-–—~]', ' ', text)
    text = re.sub(r'[^\w\s/]', '', text)
    return ' '.join(text.split())


def clean_output(raw: str) -> str:
    for marker in ["Human:", "Context:", "Question:", "Assistant:"]:
        if marker in raw:
            raw = raw.split(marker)[0]
    raw = raw.split('\n')[0].strip()
    if len(raw) > 50 and '.' in raw:
        raw = raw.split('.')[0].strip()
    return raw

def fuzzy_match(pred: str, truth: str, threshold: float = 80) -> bool:
    pred_clean = clean_output(pred)
    pred_norm = normalize_answer(pred_clean)
    truth_norm = normalize_answer(truth)
    if pred_norm == truth_norm:
        return True
    if truth_norm in pred_norm or pred_norm in truth_norm:
        return True
    if fuzz.ratio(pred_norm, truth_norm) >= threshold:
        return True
    pred_tokens = set(pred_norm.split())
    truth_tokens = set(truth_norm.split())
    if truth_tokens and truth_tokens.issubset(pred_tokens):
        return True
    return False

def exact_match(prediction, ground_truth):
    """
    助教規則：整個答案包含於輸出結果，不分大小寫，連續詞彙不可分割
    """
    pred_lower = prediction.lower().strip()
    gt_lower = ground_truth.lower().strip()
    return gt_lower in pred_lower


results = []
all_comparisons = []
correct_count = 0
recall_at_1 = 0
recall_at_5 = 0

for i, query in tqdm(enumerate(queries), total=len(queries), desc="Evaluating"):
    try:
        response = chain.invoke({"input": query})

        raw_output = response.get('answer', '').strip()
        context_docs = response.get('context', [])
        ground_truth = answers[i]

        is_correct = exact_match(raw_output, ground_truth)

        if is_correct:
            correct_count += 1

        retrieved_ids = [doc.metadata.get('id') for doc in context_docs]
        r1 = (i in retrieved_ids[:1])
        r5 = (i in retrieved_ids[:5])

        if r1:
            recall_at_1 += 1
        if r5:
            recall_at_5 += 1

        results.append({
            "Query": query,
            "Ground_Truth": ground_truth,
            "Prediction": raw_output
        })

        all_comparisons.append({
            "idx": i + 1,
            "query": query,
            "ground_truth": ground_truth,
            "raw_output": raw_output[:150],
            "is_correct": is_correct,
            "recall@1": r1,
            "recall@5": r5,
            "answer_in_context": i in retrieved_ids
        })

    except Exception as e:
        print(f"\nError Q{i+1}: {e}")
        results.append({
            "Query": query,
            "Ground_Truth": answers[i],
            "Prediction": "ERROR"
        })
        all_comparisons.append({
            "idx": i + 1,
            "query": query,
            "ground_truth": answers[i],
            "raw_output": "ERROR",
            "is_correct": False,
            "recall@1": False,
            "recall@5": False,
            "answer_in_context": False
        })

total = len(queries)
em = correct_count / total
r1 = recall_at_1 / total
r5 = recall_at_5 / total

print(f"\n{'='*60}")
print(f"FINAL RESULTS")
print(f"{'='*60}")
print(f"Total Questions: {total}")
print(f"\n--- Retrieval Metrics ---")
print(f"Recall@1: {r1:.4f} ({r1*100:.2f}%)")
print(f"Recall@5: {r5:.4f} ({r5*100:.2f}%)")
print(f"\n--- Generation Metrics ---")
print(f"Exact Match (EM): {em:.4f} ({em*100:.2f}%)")
print(f"Correct: {correct_count}/{total}")

with open('optimized_rag_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
print(f"\n✓ Saved to optimized_rag_results.json")


print("\n" + "=" * 80)
print("DETAILED COMPARISON")
print("=" * 80)
print(f"{'#':<4} {'EM':<4} {'R@5':<4} {'Ground Truth':<25} {'Prediction':<30}")
print("-" * 80)

for c in all_comparisons:
    em_mark = "✓" if c['is_correct'] else "✗"
    r5_mark = "✓" if c['recall@5'] else "✗"
    gt = c['ground_truth'][:23] + ".." if len(c['ground_truth']) > 25 else c['ground_truth']
    pred = c['raw_output'][:28] + ".." if len(c['raw_output']) > 30 else c['raw_output']
    print(f"{c['idx']:<4} {em_mark:<4} {r5_mark:<4} {gt:<25} {pred:<30}")

retrieval_errors = [c for c in all_comparisons if not c['is_correct'] and not c['answer_in_context']]
generation_errors = [c for c in all_comparisons if not c['is_correct'] and c['answer_in_context']]

print(f"\nTotal Errors: {total - correct_count}")
print(f"  - Retrieval Failures: {len(retrieval_errors)} (答案不在 context)")
print(f"  - Generation Failures: {len(generation_errors)} (答案在 context 但提取失敗)")

print("\n" + "-" * 80)
print("TOP 10 GENERATION FAILURES")
print("-" * 80)
for c in generation_errors[:10]:
    print(f"\n#{c['idx']}. Q: {c['query']}")
    print(f"    Ground Truth: {c['ground_truth']}")
    print(f"    Raw Output: {c['raw_output'][:80]}...")

print("\n" + "-" * 80)
print("TOP 10 RETRIEVAL FAILURES")
print("-" * 80)
for c in retrieval_errors[:10]:
    print(f"\n#{c['idx']}. Q: {c['query']}")
    print(f"    Ground Truth: {c['ground_truth']}")
