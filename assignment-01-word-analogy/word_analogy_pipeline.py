# -*- coding: utf-8 -*-
"""Assignment 1: Word Analogy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IqK5GuFQ7TxGSUZHlWvjQ21g6m4i05OQ

### Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntactic` category.

questions = []
categories = []
sub_categories = []

current_subcategory = None
semantic_count = 0

for line in data:
    line = line.strip()
    if not line:
        continue
    if line.startswith(":"):
        semantic_count += 1
        current_subcategory = line[1:].strip()
        continue
    if current_subcategory:
        questions.append(line)
        sub_categories.append(current_subcategory)
        if semantic_count <= 5:
            categories.append("semantic")
        else:
            categories.append("syntactic")

# Create the dataframe
df = pd.DataFrame(
    {
        "Question": questions,
        "Category": categories,
        "SubCategory": sub_categories,
    }
)

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""### Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.


"""

!pip install gensim
import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME = "glove-wiki-gigaword-100"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
      # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
    words = analogy.split()
    a, b, c, d = words[0], words[1], words[2], words[3]
    golds.append(d)

    # æª¢æŸ¥æ‰€æœ‰è©æ˜¯å¦éƒ½åœ¨æ¨¡å‹ä¸­
    if not all(word in model for word in [a, b, c]):
        preds.append(None)
        continue

    # åŸ·è¡Œå‘é‡é‹ç®—
    similar_words = model.most_similar(positive=[b, c], negative=[a], topn=1)
    prediction = similar_words[0][0]
    preds.append(prediction)

# è½‰æˆnumpy arrayæ–¹ä¾¿å¾ŒçºŒè™•ç†
golds_np = np.array(golds)
preds_np = np.array(preds)
print(f"è™•ç†å®Œæˆï¼å…± {len(golds)} å€‹é¡æ¯”å•é¡Œ")
print(f"æˆåŠŸé æ¸¬: {sum(1 for p in preds if p is not None)} å€‹")

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data["SubCategory"] == "family"]
all_words = set()

for question in family_data["Question"]:
    words = question.split()
    for word in words[:4]:
        if word in model:
            all_words.add(word)

family_words = list(all_words)

word_vectors = []
valid_words = []

for word in family_words:
    if word in model:
        word_vectors.append(model[word])
        valid_words.append(word)

word_vectors = np.array(word_vectors)

# åŸ·è¡Œt-SNEé™ç¶­
tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(valid_words)-1))
tsne_results = tsne.fit_transform(word_vectors)

plt.figure(figsize=(15, 10));
plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.4, s=90);

for i, word in enumerate(valid_words):
    plt.annotate(word,
                xy=(tsne_results[i, 0], tsne_results[i, 1]),
                xytext=(5, 5), textcoords='offset points',
                fontsize=10, alpha=0.8);

plt.title("Word Relationships from Google Analogy Task");
plt.show();
plt.savefig("word_relationships.png", bbox_inches="tight");

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""

# Download the split Wikipedia files
# Each file contain 562365 lines (articles).
!pip install gdown
!gdown --folder "https://drive.google.com/drive/folders/1RCan_ieaUW0tXeJzIJzBw_gaZAzCDzKg?usp=sharing" -O .

# Extract the downloaded wiki_texts_parts files.
!gunzip -k wiki_texts_part_*.gz

# Combine the extracted wiki_texts_parts files.
!cat wiki_texts_part_*.txt > wiki_texts_combined.txt

from google.colab import drive
drive.mount('/content/drive')

# Check the first ten lines of the combined file
!head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random

wiki_txt_path = "wiki_texts_combined.txt"
output_path = "wiki_sampled_20%_result.txt"

# é å…ˆè¨ˆç®—ç¸½è¡Œæ•¸ï¼ˆé«˜æ•ˆç‡æ–¹æ³•ï¼‰
print("æ­£åœ¨è¨ˆç®—ç¸½è¡Œæ•¸...")
with open(wiki_txt_path, "r", encoding="utf-8") as f:
    total_lines = sum(1 for _ in f)
print(f"ç¸½å…±æœ‰ {total_lines:,} ç¯‡ç¶­åŸºç™¾ç§‘æ–‡ç« ")

# è¨ˆç®—éœ€è¦æ¡æ¨£çš„æ•¸é‡ä¸¦ç”Ÿæˆéš¨æ©Ÿç´¢å¼•
target_samples = int(total_lines * 0.2)  # 20%
print(f"ç›®æ¨™æ¡æ¨£: {target_samples:,} ç¯‡æ–‡ç« ")

random.seed(42)  # è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯é‡ç¾
sample_indices = set(random.sample(range(total_lines), target_samples))

with open(wiki_txt_path, "r", encoding="utf-8") as f:
    with open(output_path, "w", encoding="utf-8") as output_file:
        # TODO4: Sample `20%` Wikipedia articles
        # Write your code here
        sampled_count = 0
        for line_idx, line in enumerate(f):
            if line_idx in sample_indices:
                output_file.write(line)
                sampled_count += 1

print(f"\n æ¡æ¨£å®Œæˆï¼å¯¦éš›æ¡æ¨£äº† {sampled_count:,} ç¯‡æ–‡ç«  ({sampled_count/total_lines*100:.2f}%)")

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec

# å„ªåŒ–ç‰ˆæœ¬ï¼šè§£æ±ºè¨˜æ†¶é«”å’Œæ•ˆèƒ½å•é¡Œ

import re
import time
import multiprocessing
from collections import Counter
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec
import numpy as np
import logging
from gensim.models.phrases import Phrases, Phraser
import gc
import psutil
import os

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def monitor_memory():
    """ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process(os.getpid())
    memory_mb = process.memory_info().rss / 1024 / 1024
    return memory_mb

def pick_vocab_form(word, wv):
    """è‡ªå‹•å°‹æ‰¾å­˜åœ¨æ–¼è©å½™è¡¨çš„è©å½™å½¢å¼"""
    candidates = [word, word.lower(), word.capitalize(), word.upper()]
    for candidate in candidates:
        if candidate in wv.key_to_index:
            return candidate
    return None

def quick_probe_consistency(wv, n=200):
    """å¿«é€Ÿæª¢æŸ¥éš¨æ©Ÿè©å½™çš„ç›¸ä¼¼åº¦è¨ˆç®—ä¸€è‡´æ€§"""
    import random
    words = list(wv.key_to_index.keys())
    sample = random.sample(words, min(n, len(words)))
    success_count = 0

    for word in sample:
        try:
            wv.most_similar(word, topn=1)
            success_count += 1
        except Exception:
            pass

    success_rate = success_count / len(sample) * 100
    print(f"éš¨æ©Ÿè©å½™ä¸€è‡´æ€§æª¢æŸ¥: {success_count}/{len(sample)} ({success_rate:.1f}%)")

    if success_rate < 95:
        print("âš ï¸ è­¦å‘Šï¼šè©å½™ä¸€è‡´æ€§è¼ƒä½ï¼Œå¯èƒ½å­˜åœ¨å‘é‡é€€åŒ–å•é¡Œ")

    return success_rate

# ä¿®æ”¹åƒæ•¸ï¼šæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ï¼Œæé«˜æ•ˆç‡
MEMORY_OPTIMIZED_PARAMS = {
    'input_file': 'wiki_sampled_20%_result.txt',
    'max_lines': None,
    'preserve_pos_patterns': True,
    'preserve_morphology': True,
    'context_quality_filter': True,
    'min_sentence_length': 8,
    'max_sentence_length': 20,  # ç¸®çŸ­å¥å­é•·åº¦
    'min_word_freq': 20,  # æé«˜é »ç‡é–€æª»ï¼Œæ¸›å°‘è©å½™é‡
    'preserve_entities': True,

    # Word2Vec åƒæ•¸ - è¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬
    'vector_size': 200,  # é™ä½ç¶­åº¦
    'window': 8,         # é™ä½çª—å£å¤§å°
    'min_count': 20,     # æé«˜æœ€å°è¨ˆæ•¸ï¼Œèˆ‡ min_word_freq ä¸€è‡´
    'epochs': 5,         # æ¸›å°‘è¨“ç·´è¼ªæ¬¡
    'sg': 1,
    'negative': 5,       # é™ä½è² æ¡æ¨£
    'sample': 1e-3,      # å¢åŠ æ¡æ¨£ç‡
    'alpha': 0.025,
    'min_alpha': 0.0001,
    'workers': max(1, multiprocessing.cpu_count() - 1),  # ä½¿ç”¨å¤šæ ¸å¿ƒ
    'batch_words': 50000,  # å¢åŠ æ‰¹æ¬¡å¤§å°

    # Phrases åƒæ•¸ - è¨˜æ†¶é«”ä¿è­·
    'enable_phrases': True,
    'phrases_min_count': 25,    # å¤§å¹…æé«˜é–€æª»
    'phrases_threshold': 50.0,  # æé«˜åˆä½µé–€æª»
    'max_phrases_memory_gb': 4,  # è¨˜æ†¶é«”é™åˆ¶

    'ns_exponent': 0.75,
    'model_name': 'memory_optimized.model',
    'vector_name': 'memory_optimized_vectors.bin'
}

class AnalogyPreprocessor:
    def __init__(self):
        self.word_pattern = re.compile(r"\b[a-zA-Z][a-zA-Z\-']{1,24}\b")

        # ç²¾ç°¡åœç”¨è©
        self.minimal_stopwords = frozenset([
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'this', 'that', 'these', 'those'
        ])

        self.entity_patterns = [
            re.compile(r'\b[A-Z][a-zA-Z]*\b'),
            re.compile(r'\b[A-Z]{2,}\b'),
        ]

        self.morphological_patterns = [
            re.compile(r'\b\w+(?:ed|ing|s|es|er|est|ly|tion|sion|ness|ment)\b', re.IGNORECASE),
        ]

    def is_entity_like(self, word):
        return word and len(word) > 1 and word[0].isupper()

    def is_morphologically_important(self, word):
        return any(pattern.match(word) for pattern in self.morphological_patterns)

    def has_semantic_value(self, word):
        word_lower = word.lower()

        if len(word) < 2 or len(word) > 20:
            return False

        if not re.match(r'^[a-zA-Z\-\']+$', word):
            return False

        if word_lower in self.minimal_stopwords:
            return False

        return True

    def preprocess_sentence(self, text):
        if not text or len(text) < 10:
            return []

        words = self.word_pattern.findall(text)
        if len(words) < MEMORY_OPTIMIZED_PARAMS['min_sentence_length']:
            return []

        processed_words = []
        for word in words:
            if self.has_semantic_value(word):
                if self.is_entity_like(word):
                    processed_words.append(word)
                else:
                    processed_words.append(word.lower())

        if len(processed_words) < MEMORY_OPTIMIZED_PARAMS['min_sentence_length']:
            return []

        if len(processed_words) > MEMORY_OPTIMIZED_PARAMS['max_sentence_length']:
            processed_words = processed_words[:MEMORY_OPTIMIZED_PARAMS['max_sentence_length']]

        return processed_words

def create_phrases_safe(sentences, params):
    """è¨˜æ†¶é«”å®‰å…¨çš„çŸ­èªå‰µå»º"""
    if not params.get('enable_phrases', True):
        print("è·³éçŸ­èªåˆä½µ")
        return sentences

    print(f"è¨˜æ†¶é«”ä½¿ç”¨: {monitor_memory():.1f} MB")

    # è¨˜æ†¶é«”æª¢æŸ¥
    if monitor_memory() > params.get('max_phrases_memory_gb', 4) * 1024:
        print("âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨éé«˜ï¼Œè·³éçŸ­èªåˆä½µ")
        return sentences

    min_count = params.get('phrases_min_count', 50)
    threshold = params.get('phrases_threshold', 50.0)

    print(f"å‰µå»ºçŸ­èª (min_count={min_count}, threshold={threshold})")

    try:
        # åˆ†æ‰¹è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”
        batch_size = 100000
        if len(sentences) > batch_size:
            print(f"å¤§èªæ–™åº«ï¼Œåˆ†æ‰¹è™•ç† (batch_size={batch_size})")

            # è¨“ç·´åœ¨å­é›†ä¸Š
            sample_sentences = sentences[:batch_size]
            bigram = Phrases(sample_sentences, min_count=min_count, threshold=threshold)
            bigram_phraser = Phraser(bigram)

            # æ‡‰ç”¨åˆ°å…¨éƒ¨æ•¸æ“š
            result_sentences = []
            for i in range(0, len(sentences), batch_size):
                batch = sentences[i:i+batch_size]
                batch_result = [bigram_phraser[sentence] for sentence in batch]
                result_sentences.extend(batch_result)

                if (i // batch_size + 1) % 5 == 0:
                    print(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(sentences)-1)//batch_size + 1}")
                    print(f"è¨˜æ†¶é«”ä½¿ç”¨: {monitor_memory():.1f} MB")

            return result_sentences

        else:
            bigram = Phrases(sentences, min_count=min_count, threshold=threshold)
            bigram_phraser = Phraser(bigram)
            return [bigram_phraser[sentence] for sentence in sentences]

    except MemoryError:
        print("âš ï¸ è¨˜æ†¶é«”ä¸è¶³ï¼Œè·³éçŸ­èªåˆä½µ")
        return sentences
    except Exception as e:
        print(f"âš ï¸ çŸ­èªè™•ç†å¤±æ•—: {e}")
        return sentences

def process_corpus_optimized(file_path, params):
    """è¨˜æ†¶é«”å„ªåŒ–çš„èªæ–™åº«è™•ç†"""
    preprocessor = AnalogyPreprocessor()
    start_time = time.time()

    sentences = []
    word_counter = Counter()
    processed_lines = 0
    memory_reports = 0

    print(f"é–‹å§‹è™•ç†èªæ–™åº«: {file_path}")
    print(f"ç›®æ¨™åƒæ•¸: min_word_freq={params['min_word_freq']}, min_count={params['min_count']}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if params['max_lines'] and processed_lines >= params['max_lines']:
                    break

                processed_words = preprocessor.preprocess_sentence(line.strip())

                if processed_words:
                    sentences.append(processed_words)
                    word_counter.update(processed_words)

                processed_lines += 1

                # è¨˜æ†¶é«”å’Œé€²åº¦ç›£æ§
                if processed_lines % 100000 == 0:
                    elapsed = time.time() - start_time
                    memory_mb = monitor_memory()
                    print(f"è™•ç† {processed_lines:,} è¡Œ -> {len(sentences):,} å¥å­ | {elapsed:.1f}s | {memory_mb:.1f} MB")

                    # è¨˜æ†¶é«”ä¿è­·
                    if memory_mb > 8000:  # 8GB é™åˆ¶
                        print("âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨éé«˜ï¼Œåœæ­¢è®€å–æ›´å¤šæ•¸æ“š")
                        break

    except Exception as e:
        print(f"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}")
        return []

    elapsed = time.time() - start_time
    print(f"\nåˆæ­¥çµ±è¨ˆ:")
    print(f"  ç¸½å¥å­æ•¸: {len(sentences):,}")
    print(f"  è©å½™ç¨®é¡: {len(word_counter):,}")
    print(f"  è™•ç†æ™‚é–“: {elapsed:.1f}s")
    print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {monitor_memory():.1f} MB")

    # çµ±ä¸€éæ¿¾é–€æª»
    min_freq = max(params['min_word_freq'], params['min_count'])
    print(f"\nä½¿ç”¨çµ±ä¸€éæ¿¾é–€æª»: {min_freq}")

    # éæ¿¾ä½é »è©
    vocab_to_keep = {word for word, freq in word_counter.items() if freq >= min_freq}

    print(f"ä¿ç•™è©å½™æ•¸: {len(vocab_to_keep):,} / {len(word_counter):,}")

    # é‡æ§‹å¥å­
    filtered_sentences = []
    for sentence in sentences:
        filtered_sentence = [w for w in sentence if w in vocab_to_keep]
        if len(filtered_sentence) >= params['min_sentence_length']:
            filtered_sentences.append(filtered_sentence)

    # è¨˜æ†¶é«”æ¸…ç†
    del sentences, word_counter
    gc.collect()

    print(f"éæ¿¾å¾Œå¥å­æ•¸: {len(filtered_sentences):,}")
    print(f"è¨˜æ†¶é«”ä½¿ç”¨: {monitor_memory():.1f} MB")

    # å®‰å…¨çš„çŸ­èªè™•ç†
    if params.get('enable_phrases', True):
        filtered_sentences = create_phrases_safe(filtered_sentences, params)
        gc.collect()
        print(f"çŸ­èªè™•ç†å¾Œè¨˜æ†¶é«”: {monitor_memory():.1f} MB")

    final_vocab_size = len(set(word for sentence in filtered_sentences for word in sentence))

    print(f"\næœ€çµ‚çµ±è¨ˆ:")
    print(f"  æœ€çµ‚å¥å­æ•¸: {len(filtered_sentences):,}")
    print(f"  æœ€çµ‚è©å½™æ•¸: {final_vocab_size:,}")
    print(f"  æœ€çµ‚è¨˜æ†¶é«”: {monitor_memory():.1f} MB")

    return filtered_sentences

class SimpleCallback(CallbackAny2Vec):
    def __init__(self):
        self.epoch = 0
        self.start_time = time.time()

    def on_epoch_begin(self, model):
        memory_mb = monitor_memory()
        print(f"é–‹å§‹ Epoch {self.epoch + 1}/{MEMORY_OPTIMIZED_PARAMS['epochs']} | è¨˜æ†¶é«”: {memory_mb:.1f} MB")

    def on_epoch_end(self, model):
        self.epoch += 1
        elapsed = time.time() - self.start_time
        memory_mb = monitor_memory()
        print(f"Epoch {self.epoch} å®Œæˆ | ç¸½æ™‚é–“: {elapsed:.1f}s | è¨˜æ†¶é«”: {memory_mb:.1f} MB")

def train_word2vec_optimized(sentences, params):
    """è¨˜æ†¶é«”å„ªåŒ–çš„ Word2Vec è¨“ç·´"""
    print(f"\né–‹å§‹è¨“ç·´è¨˜æ†¶é«”å„ªåŒ–çš„ Word2Vec æ¨¡å‹")
    print(f"  å¥å­æ•¸é‡: {len(sentences):,}")
    print(f"  å‘é‡ç¶­åº¦: {params['vector_size']}")
    print(f"  çª—å£å¤§å°: {params['window']}")
    print(f"  è¨“ç·´è¼ªæ¬¡: {params['epochs']}")
    print(f"  å·¥ä½œç·šç¨‹: {params['workers']}")
    print(f"  ç•¶å‰è¨˜æ†¶é«”: {monitor_memory():.1f} MB")

    model_params = {
        'vector_size': params['vector_size'],
        'window': params['window'],
        'min_count': params['min_count'],
        'sg': params['sg'],
        'negative': params['negative'],
        'sample': params['sample'],
        'alpha': params['alpha'],
        'min_alpha': params['min_alpha'],
        'workers': params['workers'],
        'epochs': params['epochs'],
        'batch_words': params['batch_words'],
        'callbacks': [SimpleCallback()],
        'seed': 42,
        'compute_loss': False,
        'ns_exponent': params['ns_exponent']
    }

    start_time = time.time()

    try:
        model = Word2Vec(sentences, **model_params)

        training_time = time.time() - start_time
        vocab_size = len(model.wv.key_to_index)

        print(f"\n=== è¨“ç·´å®Œæˆçµ±è¨ˆ ===")
        print(f"  è©å½™é‡: {vocab_size:,}")
        print(f"  èªæ–™å¥å­æ•¸: {model.corpus_count:,}")
        print(f"  èªæ–™ç¸½è©æ•¸: {model.corpus_total_words:,}")
        print(f"  è¨“ç·´æ™‚é–“: {training_time:.1f}s")
        print(f"  æœ€çµ‚è¨˜æ†¶é«”: {monitor_memory():.1f} MB")

        # å¿«é€Ÿå¥åº·æª¢æŸ¥
        print(f"\n=== æ¨¡å‹å¥åº·æª¢æŸ¥ ===")
        probe_words = ["the", "of", "and", "to", "a", "in", "is", "it", "you", "that"]
        print("é«˜é »è©ç›¸ä¼¼æ€§æª¢æŸ¥:")
        for w in probe_words[:5]:  # åªæª¢æŸ¥å‰5å€‹
            if w in model.wv.key_to_index:
                try:
                    similar = [word for word, _ in model.wv.most_similar(w, topn=3)]
                    print(f"  '{w}' -> {similar}")
                    break  # åªè¦ä¸€å€‹æˆåŠŸå°±OK
                except:
                    pass

        # å‘é‡å“è³ªæª¢æŸ¥
        norms = np.linalg.norm(model.wv.vectors, axis=1)
        print(f"\nå‘é‡çµ±è¨ˆ: å‡å€¼={norms.mean():.3f}, æ¨™æº–å·®={norms.std():.3f}")

        quick_probe_consistency(model.wv, n=50)  # æ¸›å°‘æª¢æŸ¥æ•¸é‡

        return model

    except Exception as e:
        print(f"è¨“ç·´éç¨‹å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•¸ - è¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬"""
    total_start = time.time()

    print("=== è¨˜æ†¶é«”å„ªåŒ–ç‰ˆ Word2Vec è¨“ç·´ ===")
    print(f"ç³»çµ±è¨˜æ†¶é«”: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    print(f"å¯ç”¨è¨˜æ†¶é«”: {psutil.virtual_memory().available / 1024**3:.1f} GB")

    try:
        # 1. è™•ç†èªæ–™åº«
        sentences = process_corpus_optimized(
            MEMORY_OPTIMIZED_PARAMS['input_file'],
            MEMORY_OPTIMIZED_PARAMS
        )

        if not sentences:
            print("æ²’æœ‰æœ‰æ•ˆçš„å¥å­ï¼ŒçµæŸç¨‹åº")
            return

        # 2. è¨“ç·´æ¨¡å‹
        model = train_word2vec_optimized(sentences, MEMORY_OPTIMIZED_PARAMS)

        if model is None:
            print("æ¨¡å‹è¨“ç·´å¤±æ•—ï¼ŒçµæŸç¨‹åº")
            return

        # 3. ç°¡åŒ–è©•ä¼°ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰
        print(f"\n=== å¿«é€Ÿé¡æ¯”æ¸¬è©¦ ===")
        simple_tests = [
            ('good', 'better', 'bad', 'worse'),
            ('king', 'queen', 'man', 'woman'),
        ]

        successful = 0
        for a, b, c, expected in simple_tests:
            vocab_a = pick_vocab_form(a, model.wv)
            vocab_b = pick_vocab_form(b, model.wv)
            vocab_c = pick_vocab_form(c, model.wv)
            vocab_expected = pick_vocab_form(expected, model.wv)

            if all(v is not None for v in [vocab_a, vocab_b, vocab_c, vocab_expected]):
                try:
                    result = model.wv.most_similar(
                        positive=[vocab_b, vocab_c],
                        negative=[vocab_a],
                        topn=3
                    )
                    predicted_words = [word for word, score in result]

                    if vocab_expected in predicted_words:
                        print(f"âœ“ {vocab_a}:{vocab_b} :: {vocab_c}:{vocab_expected}")
                        successful += 1
                    else:
                        print(f"âœ— {vocab_a}:{vocab_b} :: {vocab_c}:{vocab_expected} -> {predicted_words[0]}")
                except:
                    print(f"âœ— {vocab_a}:{vocab_b} :: {vocab_c}:{vocab_expected} - è¨ˆç®—éŒ¯èª¤")

        # 4. ä¿å­˜æ¨¡å‹
        model.save(MEMORY_OPTIMIZED_PARAMS['model_name'])
        model.wv.save_word2vec_format(MEMORY_OPTIMIZED_PARAMS['vector_name'], binary=True)

        kv_filename = MEMORY_OPTIMIZED_PARAMS['model_name'].replace('.model', '.kv')
        model.wv.save(kv_filename)

        print(f"\næ¨¡å‹å·²ä¿å­˜:")
        print(f"  å®Œæ•´æ¨¡å‹: {MEMORY_OPTIMIZED_PARAMS['model_name']}")
        print(f"  äºŒé€²åˆ¶å‘é‡: {MEMORY_OPTIMIZED_PARAMS['vector_name']}")
        print(f"  KeyedVectors: {kv_filename}")

        total_time = time.time() - total_start
        print(f"\nç¸½åŸ·è¡Œæ™‚é–“: {total_time:.1f}s")
        print(f"æœ€çµ‚è¨˜æ†¶é«”ä½¿ç”¨: {monitor_memory():.1f} MB")

    except Exception as e:
        print(f"ç¨‹åºåŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []
from gensim.models import Word2Vec
from tqdm import tqdm
import numpy as np

# è¼‰å…¥ä½ è¨“ç·´çš„æ¨¡å‹
model = Word2Vec.load("memory_optimized.model")  # ä½¿ç”¨ä½ çš„æ¨¡å‹æ–‡ä»¶å
print(f"è¼‰å…¥æ¨¡å‹å®Œæˆï¼Œè©å½™é‡: {len(model.wv.key_to_index):,}")

def pick_vocab_form(word, wv):
    """è‡ªå‹•å°‹æ‰¾å­˜åœ¨æ–¼è©å½™è¡¨çš„è©å½™å½¢å¼ï¼ˆèˆ‡ä½ çš„è¨“ç·´ä»£ç¢¼ä¸€è‡´ï¼‰"""
    candidates = [word, word.lower(), word.capitalize(), word.upper()]
    for candidate in candidates:
        if candidate in wv.key_to_index:
            return candidate
    return None

def post_process_prediction(prediction, gold_answer):
    """å¾Œè™•ç†é æ¸¬çµæœï¼Œèª¿æ•´å¤§å°å¯«ä»¥åŒ¹é…é‡‘æ¨™æº–ç­”æ¡ˆçš„æ ¼å¼"""
    if prediction is None or gold_answer is None:
        return prediction

    # å¦‚æœé‡‘æ¨™æº–ç­”æ¡ˆæ˜¯é¦–å­—æ¯å¤§å¯«ï¼Œå°‡é æ¸¬ä¹Ÿèª¿æ•´ç‚ºé¦–å­—æ¯å¤§å¯«
    if gold_answer[0].isupper():
        return prediction.capitalize()
    # å¦‚æœé‡‘æ¨™æº–ç­”æ¡ˆæ˜¯å…¨å¤§å¯«ï¼Œå°‡é æ¸¬ä¹Ÿèª¿æ•´ç‚ºå…¨å¤§å¯«
    elif gold_answer.isupper():
        return prediction.upper()
    # å¦‚æœé‡‘æ¨™æº–ç­”æ¡ˆæ˜¯å…¨å°å¯«ï¼Œå°‡é æ¸¬ä¹Ÿèª¿æ•´ç‚ºå…¨å°å¯«
    elif gold_answer.islower():
        return prediction.lower()
    else:
        # ä¿æŒåŸæ¨£
        return prediction

for analogy in tqdm(data["Question"], desc="è™•ç†é¡æ¯”ä»»å‹™"):
    try:
        # åˆ¤æ–·è¼¸å…¥æ ¼å¼ä¸¦ç›¸æ‡‰è™•ç†
        if "::" in analogy:
            # æ¨™æº–æ ¼å¼ï¼šword_A:word_B::word_C:word_D
            parts = analogy.split("::")
            if len(parts) != 2:
                preds.append(None)
                golds.append(None)
                continue

            left_part = parts[0].split(":")
            right_part = parts[1].split(":")

            if len(left_part) != 2 or len(right_part) != 2:
                preds.append(None)
                golds.append(None)
                continue

            word_A, word_B = left_part[0].strip(), left_part[1].strip()
            word_C, word_D = right_part[0].strip(), right_part[1].strip()

        else:
            # ç©ºæ ¼åˆ†éš”æ ¼å¼ï¼šword_A word_B word_C word_D
            words = analogy.split()
            if len(words) != 4:
                preds.append(None)
                golds.append(None)
                continue
            word_A, word_B, word_C, word_D = words[0], words[1], words[2], words[3]

        # ä¿å­˜é‡‘æ¨™æº–ç­”æ¡ˆ
        golds.append(word_D)

        # ä½¿ç”¨èˆ‡è¨“ç·´ä»£ç¢¼ä¸€è‡´çš„è©å½™æŸ¥æ‰¾æ–¹å¼
        vocab_A = pick_vocab_form(word_A, model.wv)
        vocab_B = pick_vocab_form(word_B, model.wv)
        vocab_C = pick_vocab_form(word_C, model.wv)

        # æª¢æŸ¥æ‰€éœ€è©å½™æ˜¯å¦éƒ½åœ¨æ¨¡å‹ä¸­
        if not all(word is not None for word in [vocab_A, vocab_B, vocab_C]):
            preds.append(None)
            continue

        # ä½¿ç”¨å‘é‡é‹ç®—: word_D â‰ˆ word_B - word_A + word_C
        try:
            similar_words = model.wv.most_similar(
                positive=[vocab_B, vocab_C],
                negative=[vocab_A],
                topn=10  # å–æ›´å¤šå€™é¸ï¼Œæé«˜æº–ç¢ºç‡
            )

            # å–æœ€ç›¸ä¼¼çš„è©ä½œç‚ºé æ¸¬
            predicted_word = similar_words[0][0]

            # ğŸ¯ æ–°å¢ï¼šå¾Œè™•ç†é æ¸¬çµæœï¼Œèª¿æ•´å¤§å°å¯«æ ¼å¼
            processed_prediction = post_process_prediction(predicted_word, word_D)
            preds.append(processed_prediction)

        except KeyError:
            preds.append(None)
        except Exception as e:
            print(f"è™•ç†é¡æ¯” '{analogy}' æ™‚å‡ºéŒ¯: {e}")
            preds.append(None)

    except Exception as e:
        print(f"è§£æé¡æ¯” '{analogy}' æ™‚å‡ºéŒ¯: {e}")
        preds.append(None)
        golds.append(None)

# çµ±è¨ˆçµæœ
total_questions = len(preds)
valid_predictions = sum(1 for p in preds if p is not None)
correct_predictions = sum(1 for p, g in zip(preds, golds) if p is not None and g is not None and p == g)

print(f"\né¡æ¯”ä»»å‹™çµ±è¨ˆ:")
print(f"ç¸½é¡Œæ•¸: {total_questions}")
print(f"æœ‰æ•ˆé æ¸¬: {valid_predictions} ({valid_predictions/total_questions*100:.1f}%)")
if valid_predictions > 0:
    print(f"æº–ç¢ºé æ¸¬: {correct_predictions} ({correct_predictions/valid_predictions*100:.1f}% of valid)")
print(f"æ•´é«”æº–ç¢ºç‡: {correct_predictions/total_questions*100:.1f}%")

# é¡¯ç¤ºä¸€äº›ä¾‹å­
print(f"\né æ¸¬ç¯„ä¾‹ (å‰10å€‹æœ‰æ•ˆé æ¸¬):")
shown = 0
for i in range(len(preds)):
    if preds[i] is not None and golds[i] is not None and shown < 10:
        status = "âœ“" if preds[i] == golds[i] else "âœ—"
        print(f"{status} é æ¸¬: {preds[i]}, æ­£è§£: {golds[i]}")
        shown += 1

# è½‰æ›ç‚ºnumpy arrayï¼ˆå¦‚æœéœ€è¦å¾ŒçºŒè™•ç†ï¼‰
golds_np = np.array(golds)
preds_np = np.array(preds)

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`

# Collect words from Google Analogy dataset
import matplotlib.pyplot as plt
import numpy as np
from sklearn.manifold import TSNE
from gensim.models import Word2Vec

# è¼‰å…¥ä½ çš„è¨“ç·´æ¨¡å‹
model = Word2Vec.load("memory_optimized.model")
print(f"è¼‰å…¥æ¨¡å‹å®Œæˆï¼Œè©å½™é‡: {len(model.wv.key_to_index):,}")

def pick_vocab_form(word, wv):
    """å°‹æ‰¾å­˜åœ¨æ–¼è©å½™è¡¨çš„è©å½™å½¢å¼"""
    candidates = [word, word.lower(), word.capitalize(), word.upper()]
    for candidate in candidates:
        if candidate in wv.key_to_index:
            return candidate
    return None

# ä½¿ç”¨ SubCategory éæ¿¾ family é¡åˆ¥
family_data = data[data["SubCategory"] == "family"]
print(f"æ‰¾åˆ° {len(family_data)} å€‹ family é¡åˆ¥çš„å•é¡Œ")

# é¡¯ç¤ºä¸€äº›æ¨£æœ¬
print("Family é¡åˆ¥å•é¡Œæ¨£æœ¬:")
for i, question in enumerate(family_data["Question"][:5]):
    print(f"  {i+1}. {question}")

# æ”¶é›†æ‰€æœ‰åœ¨æ¨¡å‹ä¸­å­˜åœ¨çš„è©å½™
all_words = set()
for question in family_data["Question"]:
    words = question.split()
    for word in words[:4]:  # åªå–å‰4å€‹è©
        model_word = pick_vocab_form(word, model.wv)
        if model_word is not None:
            all_words.add(word)  # ä¿å­˜åŸå§‹è©å½™å½¢å¼ç”¨æ–¼é¡¯ç¤º

family_words = list(all_words)
print(f"æ”¶é›†åˆ° {len(family_words)} å€‹åœ¨æ¨¡å‹ä¸­å­˜åœ¨çš„è©å½™")

# æº–å‚™è©å‘é‡
word_vectors = []
valid_words = []

for word in family_words:
    model_word = pick_vocab_form(word, model.wv)
    if model_word is not None:
        word_vectors.append(model.wv[model_word])
        valid_words.append(word)

print(f"æœ€çµ‚æœ‰æ•ˆè©å½™æ•¸: {len(valid_words)}")
print(f"æœ‰æ•ˆè©å½™: {valid_words[:10]}{'...' if len(valid_words) > 10 else ''}")

# åŸ·è¡Œ t-SNE é™ç¶­
word_vectors = np.array(word_vectors)
perplexity = min(30, len(valid_words)-1)
tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, max_iter=1000)
tsne_results = tsne.fit_transform(word_vectors)

# å‰µå»ºå¯è¦–åŒ–
plt.figure(figsize=(15, 10))
plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.6, s=100, c='skyblue', edgecolors='navy')

# æ·»åŠ è©å½™æ¨™ç±¤
for i, word in enumerate(valid_words):
    plt.annotate(word,
                xy=(tsne_results[i, 0], tsne_results[i, 1]),
                xytext=(5, 5), textcoords='offset points',
                fontsize=11, alpha=0.8,
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))


plt.xlabel("t-SNE Dimension 1", fontsize=12)
plt.ylabel("t-SNE Dimension 2", fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")
